# 一、数据的重要性
&emsp;&emsp;在深度学习中，算法架构的设计往往依赖于特定领域的数据特点（如卷积神经网络适配图像的空间信息，自动编码器适配含噪数据）。因此，理解自然语言处理（NLP）领域的算法前，需先掌握其数据的核心特征——**序列数据**。
# 二、序列数据
## 2.1 定义
样本间存在特定顺序，且顺序不可随意修改的数据。
## 2.2 与普通二维表数据的区别
1. 普通二维表数据：
	- 样本独立
	- 顺序不影响含义（如训练 1 号或 7 号样本，结果无本质差异）
2. 序列数据：
	- 顺序调换或样本缺失会导致含义剧变（如文本中词语顺序改变、时间序列中时间点颠倒）
## 2.3 常见类型
### 2.3.1 文本数据（Text Data）
1. 定义：文本数据中的样本的“特定顺序”是**语义**顺序，也就是词与词、句子与句子、段落与段落之间的顺序。在语义环境中，词语顺序的变化或词语的缺失可能会彻底改变语义。
2. 示例：
	- **改变顺序**：事半功倍和事倍功半；曾国藩战太平天国时非常著名的典故：他将“屡战屡败”修改为“屡败屡战”，前者给人绝望，后者给人希望。
	- **样本缺失**（对文本来说特指上下文缺失）：小猫睡在毛毯上，因为它很__。当我们在横线上填上不同的词时，句子的含义会发生变化。
### 2.3.2 时间序列数据（Time Series Data）
1. 定义：时间序列数据中的“特定顺序”就是**时间**顺序，时序数据中的每个样本就是每个时间点，在不同时间点上存在着不同的标签取值，且这些标签取值常常用于描述某个变量随时间变化的趋势，因此样本之间的顺序不能随意改变。
2. 示例：
	- 股票价格
	- 气温记录
	- 心电图
### 2.3.3 音频数据（Audio Data）
1. 定义：音频数据大部分时候是文本数据的声音信号，此时音频数据中的“特定顺序”也是**语义**顺序；当然，音频数据中的顺序也可能是**音符**顺序，试想你将一首歌的旋律全部打乱再重新播放，那整首歌的旋律和听感就会完全丧失。
### 2.3.4 视频数据（Video Data）
1. 定义：视频数据本质就是由**一帧帧图像**构成的，因此视频数据是图像按照特定顺序排列后构成的数据。和音频数据类似，如果将动画或电影中的画面顺序打乱再重新播放，那没有任何人能够理解视频的内容。
### 2.3.5 其他
- DNA 序列（测序顺序不可乱）
- 符号序列（密码学、编程逻辑依赖顺序）
# 三、时间序列数据
## 3.1 核心
核心是**时间步（time_step，即序列长度）**，时间点间隔需一致。
## 3.2 常见数据类型
### 3.2.1 二维时间序列
![](https://i-blog.csdnimg.cn/img_convert/f77c37676a444664bc21a5a50a4c0984.png)
1. 结构：
	- 单个时间序列
	- 维度为「时间步 × 特征」
2. 核心：时间步顺序是算法需学习的核心（如预测未来股价需依赖历史时间点的顺序）
### 3.2.2 三维时间序列
![](https://i-blog.csdnimg.cn/img_convert/5732b11e69120d470041e904a20dfa3a.png)
1. 结构：
	- 多个二维时间序列的集合
	- 维度为「batch_size（样本量）× 时间步 × 特征」
2. 核心：多变量时间序列（如 “时间 + 股票 ID” 共同决定某一时间点的股价）
# 四、文字序列数据
## 4.1 核心
核心是**语义顺序**，最小单位为词或字，序列长度由词汇量（vocab_size）决定。
## 4.2 常见数据类型
### 4.2.1 二维文字序列
![](https://i-blog.csdnimg.cn/img_convert/f4c4ba93cd1560c4204a7350257193de.png)
1. 结构：
	- 单个句子或段落
	- 维度为「词汇量 × 特征」（如一句话中的字词序列）
2. 核心：词汇量顺序，即语义顺序（如 “我吃饭” 不可改为 “饭吃我”）
### 4.2.2 三维文字序列
![](https://i-blog.csdnimg.cn/img_convert/bb6838004113dcc6567e02c3fcf54229.png)

![](https://i-blog.csdnimg.cn/img_convert/1f6b677b83d46c58323772f72ebb978d.png)
1. 结构：
	- 多个句子或段落的集合
	- 维度为「batch_size× 词汇量 × 特征」
## 4.3 分词操作
### 4.3.1 定义
将连续的文本切分成一个个具有独立意义的词或词组的过程。
### 4.3.2 意义
原始文本数据大多是段落，但是输入到深度学习中的文字数据的样本却是词或字，因此文字数据大部分时候需要进行“分词”。良好的分词可以降低算法理解文本的难度，可以很好地提升模型的性能。
### 4.3.3 手段
1. 中文分词：
	- 经典方法：
		- **基于词典的方法**：最经典的方法，例如最大匹配法、最小匹配法等，它们基于预先定义的字典来执行分词，在使用之前需要先构造词典。
		- **基于统计的方法**：例如HMM（隐马尔科夫模型）和CRF（条件随机场）。
		- **深度学习方法**：例如基于Bi-LSTM的分词模型。
	- 经典工具：
		- `jieba`：是一个Python的第三方库。支持三种分词模式：精确模式、全模式和搜索引擎模式。速度相对较快，使用简单，可以自定义添加词典。
		- `HanLP`：不仅仅是分词工具，还包括词性标注、命名实体识别等多种NLP任务。支持多种分词算法，如CRF、感知机等。同时支持繁体中文和简体中文。提供了丰富的预处理和后处理功能。
		- `THULAC`（清华大学THU词法分析工具包）：不仅支持分词，还提供词性标注功能。使用条件随机场（CRF）模型。
		- `FudanNLP`（复旦大学NLP工具集）：提供了分词、词性标注、命名实体识别等功能。使用结构化感知机模型。
		- `LTP`（语言技术平台）：由哈工大社会计算与信息检索研究中心开发。提供全套中文NLP处理工具，包括分词、词性标注、句法分析等。使用感知机模型。
		- `SNLP` (Stanford NLP for Chinese) ：斯坦福大学开发的NLP工具，支持多种语言，其中包括中文。提供分词、词性标注、命名实体识别等功能，基于都使用基于深度学习的方法。
2. 英文分词：
	- **空白字符分词**：由于英文单词之间通常由空格分隔，所以简单的空格分词在很多情况下都很有效。
	- **基于规则的方法**：如NLTK、spaCy等工具提供的分词方法。
	- **子词分词**：如BPE或SentencePiece，它们可以将英文单词进一步切分成常见的子词或字符级别的片段。
## 4.4 词、字与Token
### 4.4.1 定义
Token是==当前分词模式下的最小语义单元==，根据分词方式的不同，它可能是一个单词（upstair）、一个半词（up，stair）或一个字母（u,p,s,t...），也可能是一个短语（攀登高峰）、一个词语（攀登、高峰）或一个字（攀，登，高，峰）。
### 4.4.2 意义
1. Token的数量代表了样本的数量，也就代表了当前文本的长度和当前算法需要处理的数据量，直接对当前算法运行需要多少资源（算力、时间、电力）产生影响。
2. 大语言模型（如 GPT）的计费与限制单位（OpenAI 以 Token 计量使用量）。
## 4.5 编码
### 4.5.1 本质
编码的本质是==用单一数字或一串数字的组合去代表某个字/词==，在同一套规则下，同一个字会被编码为同样的序列或同样的数字，而使用一个数字还是一串数字则可以由算法工程师自行决定。
### 4.5.2 常见编码形式
1. **One-hot编码**：将每个词表示为一个长向量，这个向量的维度等于词汇表的大小，其中只有一个维度的值为1，其余为0。这个1的位置对应于该词在词汇表中的位置。
2. **词嵌入（Word Embeddings）**：词嵌入是一种将词或短语映射到高维空间的表示方法，使得语义上相似的词在这个高维空间中彼此接近。例如，通过训练得到的词嵌入模型，我们可能发现"king"和"queen"、"man"和"woman"在向量空间中是相似的。词嵌入通常通过大量的文本数据训练得到，目的是捕捉单词之间的语义关系。
	- 经典方法：
		- `Word2Vec`：通过神经网络模型学习单词的向量表示，常见的有CBOW和Skip-Gram两种模型。
		- `GloVe`（Global Vectors for Word Representation）：基于单词共现统计信息来学习单词的向量表示。
		- `FastText`：与Word2Vec类似，但考虑了单词内部的子词信息。
		- **固定编码**：例如使用BERT、GPT等预训练的深度学习模型来编码文本。这些模型通常使用大量数据进行预训练，并可以为新任务进行微调。
		- **基于大语言模型进行编码**：在OpenAI研发的大模型生态矩阵当中，存在专用于构建语义空间的embeddings大模型。
3. **TF-IDF**：基于单词在文档中的频率和在整个数据集中的反向文档频率来为单词分配权重。
4. **Byte Pair Encoding (BPE) / SentencePiece**：这是子词级别的编码，能够处理词汇外的单词和多种语言的文本。
5. **ElMo**：深度上下文化词嵌入，考虑了单词的上下文信息来生成词向量。
6. **Seq2Seq等序列变化模型**：如果将“编码”的概念拓展到“如何将非结构化数据（如文本）转换为结构化的数字表示”，那seq2seq等序列转化模型也可以作为编码的手段之一。seq2seq，即sequence-to-sequence，是一个在多种NLP任务中广泛应用的神经网络结构，常常被用在机器翻译、文本摘要、问答系统等需要从一个序列生成另一个序列的任务中，因此seq2seq本质与encoder-decoder非常相似，是输入序列、输出序列的模型。seq2seq模型的编码器涉及到了与文本编码很类似的过程，它将输入序列（如一个句子）转换为一个固定大小的向量。但这个向量通常是为了特定的seq2seq任务（如翻译）而生成的，并不是用于一般的文本表示。因此，虽然seq2seq的编码器确实进行了“编码”，但它和我们之前讨论的文本编码方法（如Word2Vec或TF-IDF）有些不同。

------
==微语录：没有脚踏实地建立起来的东西，就无法形成精神和物质上的支撑。——东野圭吾==
